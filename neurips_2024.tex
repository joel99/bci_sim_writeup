\documentclass{article}

\usepackage[preprint]{neurips_2024}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{microtype}
\usepackage{natbib}
\usepackage{graphicx}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\figref}[1]{Fig.~\ref{#1}}


\title{A NeuroAI-derived Brain-Computer Interface Simulator}

\author{%
  Joel Ye \\
  Carnegie Mellon University\\
  \texttt{joelye@andrew.cmu.edu} \\
  \And
  Chris Ki \\
  Carnegie Mellon University\\
  \texttt{cski@andrew.cmu.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Brain-computer interfaces (BCIs) are a promising technology for enabling communication and control of computers using the brain.
  However, the development of BCIs is currently limited by the expense and inaccessibility of real world experiments, which motivates the development of a simulation platform to evaluate BCI algorithms.
  Here, we explore a strategy for simulating user control and adaptation to different BCI decoders with a deep network controller that adapts with deep reinforcement learning (RL).
  Critically, this method avoids any reliance on pre-existing neural datasets, allowing for the evaluation of BCI algorithms in a fully synthetic environment.

  We show that RL-trained controllers adapt to linearly perturbed decoders in a 2D cursor task, matching native-control performance when decoders preserve structure and failing when correlations are destroyed.

  Our exploration here demonstrates controllability evaluation without neural recordings, but with many limitations that we discuss in post.
\end{abstract}


\section{Introduction}
\label{sec:intro}

Intracortical brain-computer interfaces (iBCIs) are an emerging technology where that convert a user's recorded neural activity into predictions of the user's behavioral intents. With good predictions, such BCIs can be used to control cursors, keyboards, or assistive devices~\citep{willett_23_speech,collinger2013high,pandarinath2017high}. iBCIs that use electrodes directly implanted in the motor cortex show particularly promising performance, as they record high signal quality neural activity that can even be modeled with simple linear decoders.
Naturally, over two decades of research, researchers have proposed many decoders that achieve higher predictive performance on offline, static datasets. This has only accelerated in recent years with the advent of larger neural datasets and new deep learning techniques~\citep{ye2025ndt3,azabou2024unified}.
Nonetheless, advances in the decoders used for online control have been much slower~\citep{temmar2024artificial}.
The major reason for this disparity is the inaccessibility and expense of real-world experiments.
iBCIs are still restricted to pre-clinical or clinical research and are thus studied in only a few dozen institutions worldwide.
In each of these institutions, experiments work with one or two users for only a few hours a week while coordinating many different research priorities.
There is a strong need for more accessible evaluation platforms for different BCI decoders, that also address more aspects of the BCI control problem.
Chief among these is the simple restatement that BCI is not a prediction problem, but a closed-loop control problem~\citep{pandarinath_22_review,deo_23_impulse}, as the user changes their intentions to compensate for decoder errors, both within a single trial and over the timescale of a single day~\citep{sadtler2014neural}.

% Topic: Our goal in this work is to develop a simulation platform that can evaluate the controllability of BCI decoders post-adaptation.
% - This entails two targets for emulation: the user's motor cortical intention, as reflected in their neural activity, and the user's adaptation to a new decoder.
% - Our work makes a major assumption on the nature of these two targets, given that our understanding of them are both highly incomplete.
% - Specifically, we assume that BCI users come to control their BCI plants by repurposing their native motor pathways, and by reassociating in a BCI specific manner which observations should pair with which native neural activity patterns to accomplish a task. Concretely, this leads us to a formulation of BCI control as a deep network controller that has its native action pathway supplanted by a BCI decoder that reads from its hidden state, as in~\citep{Menendez2025LowDimControlTheory,Feulner2021NeuralManifoldPlasticity,Humphreys2022GradientBCI}.
% - While our assumption is certainly not correct in the details, we believe it operationalizes an existing theory of BCI learning, as discussed in the next section.
% - Even less understood is the learning mechanisms driving biological learning under BCI control. Here, our modeling choice is limited by the available tooling: local perturbation and hill climbing are too inefficient for realistic control timescales, and in-context learning is too expensive, which leaves us with reinforcement learning as the only viable option.
% - Though there is work indicating that the solutions learned by RL are distinctive~\citep{lindsay2021divergent}, we take some solace in the opposing, stabilizing force that solutions are more stable in their behavioral and representational generalization under more ocnstraints~\citep{huang2025degeneracy}, particularly when initialized from a pretrained network~\citep{Humphreys2022GradientBCI,neyshabur2017exploring}.
% - As we will see, even with these strong assumptions that restrict our search space to RL-based adaptation, there are still open-ended design choices that may prevent replicating real-world experiments.
Our goal in this work is to develop a simulation platform that can evaluate the controllability of BCI decoders after adaptation. Doing so entails emulating two targets: the user’s motor-cortical intention, as reflected in their neural activity, and the user’s adaptation to a new decoder. Our approach therefore makes a major assumption about the nature of both targets, given that our understanding of each remains highly incomplete. Specifically, we assume that BCI users come to control their BCI plants by repurposing their native motor pathways and by learning a BCI-specific reassociation between task-relevant observations and native neural activity patterns. Concretely, this leads us to formulate BCI control as a deep network controller whose native action pathway is supplanted by a BCI decoder that reads out from its hidden state, as in~\citep{Menendez2025LowDimControlTheory,Feulner2021NeuralManifoldPlasticity,Humphreys2022GradientBCI}. While this assumption is certainly not correct in the details, we believe it operationalizes an existing theory of BCI learning, as discussed in the next section. Even less understood are the learning mechanisms driving biological learning under BCI control. Here, our modeling choice is limited by the available tooling: local perturbation and hill climbing are too inefficient for realistic control timescales, and in-context learning is too expensive, leaving reinforcement learning as the only viable option. Though there is work indicating that the solutions learned by RL are distinctive~\citep{lindsay2021divergent}, we take some solace in the opposing, stabilizing force that solutions become more stable in their behavioral and representational generalization under stronger constraints~\citep{huang2025degeneracy}, particularly when initialized from a pretrained network~\citep{Humphreys2022GradientBCI,neyshabur2017exploring}. As we will see, even with these strong assumptions that restrict our search space to RL-based adaptation, there remain open-ended design choices that may prevent faithful replication of real-world experiments.

% (JY: Ideally we discuss the terms in the title e.g. NeuroAI and motivate independence from neural data but this is no final project writeup.)

Contributions and concrete content and takeaways.
We aim to evaluate one slice of the BCI problem---how readily a user can regain control under a new decoder---without any recorded neural data. To do so, we replace the human with a deep network controller trained with reinforcement learning, and we replace human adaptation with additional RL fine-tuning. This approach targets controllability rather than neural realism, seeking to expose which decoders are learnable in closed-loop control.
We present an initial simulator based on a lightweight cursor-control environment, a recurrent policy, and a family of linear decoders learned from policy state. It allows rapid testing of decoder perturbations and produces behavioral readouts (trajectories, success rates) analogous to human BCI tuning sessions.

\section{Related Work}
There are three lines of related work for this project: the use of neural networks are testbeds for theories of BCI learning, the use of RL to train normative models of motor cortex, and most relevantly, the use of simulation for BCI evaluation. The first two relate mostly to our proposed method, and the third relates to our motivation.

\subsection{Neural Networks as Testbeds for BCI Learning}
While there is extensive literature in both neuroscience and machine learning discussing learning in the brain overall, the literature spanning learning in brain-computer interface use is much sparser. For an initial empirical account, \citet{sadtler2014neural} provided a seminal account of how monkeys could much more readily adapt to decoders that read out of the native neural manifold, or the subspace spanned by the neural activity's top factors of variability, than to decoders that read out-of-manifold. Control could be re-established after perturbation in-manifold perturbations within an experimental session, in 100s of trials, but not for out-of-manifold perturbations. \citet{oby2019new} later showed that the latter decoders were eventually learnable over much longer timescales, with significant animal curriculum training. In our simulator, we are mainly interested in accounting for the former, short-timescale BCI learning, rather than highly-costly learning that comes from long term BCI use. Theoretical accounts of BCI learning have since been proposed to explain these phenomena that also reconcile with classic accounts of rodents learning to modulate single units of activity~\citep{fetz1973operantly}. \citet{Menendez2025LowDimControlTheory} elaborates on the ``re-aiming'' hypothesis, that BCI users learn to control low-dimensional motor commands input to motor cortex to optimize task performance. Since the intrinsic connectivity of motor cortex, which constrains the possible neural activity patterns that the BCI sees, is not changed, BCIs that leverage out-of-manifold patterns are much harder to learn. \citet{Humphreys2022GradientBCI} provides an alternate account by showing that out-of-manifold difficulty is compatible with a whole spectrum of gradient-based local-weight updates in the network, whether the rules are the biologically implausible backpropagation or more plausible hill-climbing algorithms. \citet{Humphreys2022GradientBCI} operationalizes backpropagated learning through vector errors deriving directly from the BCI readout, which we deem implausible, and we argue (without experimental evidence) possibly too expressive to enable discrimination of different qualities of BCI decoders. \citet{Menendez2025LowDimControlTheory} does not offer an explicit learning algorithm that the brain would use to optimize its low-D inputs. Here, we explore a re-interpretation of the re-aiming hypothesis wherein the agent optimizes its native action space, rather than abstracted motor commands, which then allows the use of standard deep RL algorithms. This again, is not a statement about what happens in the brain.

\subsection{Reinforcement Learning to Train Normative Models of Motor Cortex}

There is more consensus that training recurrent neural networks to perform aniimal-ethological tasks can induce representations that correlate with neural activity from those animals performing the same tasks. These works have led to a modern view of primary motor cortex as reflecting the dynamical computation necessary to control the body, rather than (just) directly representing motor variables which are then read out by downstream networks. \citep{sussillo2015neural} showed that training RNNs to produce muscle activity timeseries from different categorical inputs yielded better neural predicitivity than directly regressing neural activity from kinematics. \citep{russo2018motor} provided an account of how motor cortex acts to `untangle' the state trajectories necessary to produce different muscle activity patterns, i.e. allowing for, approximately speaking, a smooth dynamical flow field to produce non-smooth transitions in output. \citep{almani2024musim,vargas2025imitation} show that, in absence of explicit muscle recordings, training RNNs to control musculoskeletal models to reproduce animal kinematics also induces representations that best correlate with neural activity. These works are relevant here in that they provide us neural data-free, ``normative'' models of neural activity, which we hope to interact with BCI decoders in a manner that generalizes to real world experiments.


\subsection{Simulation for BCI Evaluation}
Finally, we have the most direct predecessor work that seeks to create surrogate platforms for evalution of BCI control. For tractability, early works evaluated the controllability of specific classes of linear BCI decoders~\citep{Lagang2013StochasticOptimalControlBMI,Willett2019PrincipledBCIDecoderDesign}, though these have limited relevance given the increasing prevalence of nonlinear decoders. \citet{Cunningham2011ClosedLoopHumanSimulator} use human controllers and adaptation and synthesize synthetic neural data from their kinematics, but this still retains the crucial bottleneck of human interaction. \citet{Liang2024BCISimulator}, as most directly inspired our work, removed this limit by using RL-trained RNNs to control different BCI decoders.
However, both these prior works which rely on synthetic neural data generation still require access to neural datasets. While this is not prohibitive in it of itself, it introduces a circular dependency: the neural data encoders are trained on the same data as the decoders they seek to discriminate.
While such per-dataset calibration certainly has benefits, in replicating the signal-to-noise ratio of any given real world experiment, it is also liable to biasing the evaluation towards decoders that resemble the encoder method, akin to the challenges of LLM-derived metrics~\citep{krumdick2025nofreelabels,chen-etal-2024-humans}. Our work aims to minimize any explicit fit to neural data.

\section{Methods}
\label{sec:methods}


\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/native_surgery_hor.png}
  \caption{During adaptation, the environment acts on BCI decoder outputs while PPO updates register native actions.}
  \label{fig:native_surgery}
\end{figure}

Our approach requires specifying both a native policy POMDP and a BCI-setting POMDP~\figref{fig:native_surgery}. To the agent, these should resemble similar tasks, so they share observation spaces and task structure. Our goal is to create a platform that can evaluate the controllability of decoders for a wide range of movement BCIs, which today is progressing towards high-degree of freedom arm and hand control. Like almost all works in the field, though, we begin with a simple, 2D endpoint-control task. Our project did not advance this point (yet!).

\textbf{The native policy} can move the endpoint either directly through kinematic control (velocity or acceleration) or indirectly through a musculoskeletal model (we use the 6-muscle arm in MotorNet~\citep{Codol2024MotorNet}). \textbf{The BCI setting} uses BCI actions to move the endpoint explicitly through velocity control, as is standard in most movement BCIs. The training task is to move the endpoint to a goal position, with some acceptance radius around the goal, within the time limit. In both settings, the observation space is time-delayed and lightly noised information about current endpoint position and target endpoint position. When controlling the musculoskeletal model, proprioceptive information about muscle activations is also provided.

We shape initial training with a distance to goal reduced reward, action magnitude penalty, slack penalty, and a terminal reward for reaching the goal (hyperparameters as specified in the codebase). The policy is either an MLP or LSTM (mostly LSTM), and we borrow heavily from the Stable Baselines3 implementation of PPO~\citep{stable-baselines3}.

The actor and critic models are carried from native pretraining to the BCI adaptation period. With the expectation the BCI decoder performance is intended to approximate native motor intent, the critic is left untouched. In contrast, the policy has a BCI decoder that reads out from the final hidden activation. The agent can at this point only sensibly act in a ``dual-plant'' environment, where it first sees composite information. The first plant is the native plant, which moves for example from muscle forces as before. The second plant is a BCI plant, which moves by velocity commands as predicted from the BCI decoder. The agent observes information about the task endpoint from the BCI plant, and propioceptive information from its native plant. In principle, we can still provide information about the native plant endpoint to the policy, but we omit it as it is not relevant for the task.

In best analogy to biological implants, the BCI should be able to be inserted anywhere, not just at the penultimate layer. However, in order for the RL adaptation to even plausibly work, we require a common source of stochasticity to drive both native and BCI actions. To understand this, consider that standard RL algorithms will noise the action once it is fully sampled from the actor. Unchanged, the BCI actions will be deterministic functions of the observation, and the policy cannot discover better actions through the RL exploration process. If we noised the BCI action independently, the RL updates can spuriously associate native actions with BCI actions that coincidentally achieved high performance. Accepting the need for a common source of noise, we also constrain the minimal BCI readin point to be downstream of the noise. However, if the noise is inserted anywhere significantly upstream, we will quickly lose the ability to compute exact likelihoods as noise passes through several nonlinearities to produce any sampled action. We thus are left with no particular choice, actually, in where we can place the BCI decoder, as the penultimate, pre-linear readout is the only location that satisfies both the need for common noise and likelihood tractability. Note the BCI itself need not be differentiable or known the RL algorithm beyond a black-box interface.

With these preliminaries, our experiments use the following protocol:
\begin{enumerate}
    \item \textbf{Native control training.} We train a recurrent policy (MLP + LSTM) with PPO in a 2D cursor-reaching task (targets are randomly spawned). The resulting policy provides the ``neural population'' whose hidden state is fed to a BCI decoder. To ease transition to BCI control, we often already perform this step by injecting high-D noise in the penultimate hidden state, rather than the action space readout, as else we'll have an unstable new exploration term to tune.
    \item \textbf{Decoder fitting.} We collect trajectories from the pretrained policy in a controlled evaluation environment, and fit a linear decoder that maps hidden state to the BCI-control action -- namely, endpoint velocity -- as collected from privileged environment state (not the lagged policy observations). Decoders can be perturbed beyond this point to stress controllability.
    \item \textbf{Adaptation.} We may resume training in the dual-plant environment. Tasks may be made easier for BCI control. We continue PPO updates on the native policy head, injecting noise at the penultimate hidden state so that trajectories explore while preserving tractable log-probabilities. This simulates user adaptation without granting gradients through the decoder.
\end{enumerate}

\section{Results}
\label{sec:results}

Native policies reliably converge to completely solving the 2D reach whether the endpoint is velocity or acceleration controlled (despite sensorimotor delay). The 6-muscle arm converges below full task-solves for yet unclear reasons. We had trouble tuning the precise hyperparameters on this plant for high performance. After 20M environment steps, the policy does not converge to full solves, but learning does begin to saturate around 90\% success rate.


\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/open_loop.png}
  \caption{Sample evaluation trajectories of the policy under native behavior and under BCI open-loop control, for the control schemes explored in this work. (Velocity control omitted due to code regression.)}
  \label{fig:native_behavior}
  \end{figure}


We show sample evaluation trajectories in the same center-out environment. Note the accessible region of the workspace for the 6-muscle arm is not radial, so we restrict the task workspace for that plant to approximate the largest accessible circle. Sample trajectories of the policy under native behavior are shown in~\figref{fig:native_behavior}. Overlaid on these are open-loop BCI control trajectories, where the trajectory is generated by accumulating BCI velocity predictions from the policy's hidden state in a native trajectory. High quality alignment here suggests that if the policy could suppress its own observation of BCI decoding error, it could plausibly still control the plant with BCI decoding. Note that the velocity decoding scores are very poor for the more complex plants, considering we have full observability of the network state. This suggests that we need to revisit our modeling setup to even begin with the premise of high velocity decoding accuracy.

\subsection{Dual-plant policies reliably recover from visuomotor rotations}.
Decoders that rotate the native linear readout by $45^\circ$ or $90^\circ$ relearn quickly, recovering native-level trajectories after a short tuning phase (Figure~\ref{fig:90_tune}). A $180^\circ$ rotation is learnable but substantially slower. These behaviors mirror human experiments that measure how motor cortex compensates for decoder perturbations.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.32\linewidth]{figures/90_tune_0.png}\hfill
  \includegraphics[width=0.32\linewidth]{figures/90_tune_50.png}\hfill
  \includegraphics[width=0.32\linewidth]{figures/90_tune_100.png}
  \caption{Example trajectories during adaptation to a $90^\circ$ rotated decoder: initial (left), midway (center), and converged (right).}
  \label{fig:90_tune}
  \end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/wmp_success.png}
  \caption{Tuning curves for rotated decoders ($45^\circ$, $90^\circ$, $180^\circ$) and a random decoder. Rotations relearn; the random decoder stays near chance.}
  \label{fig:wmp_success}
  \end{figure}

\subsection{Dual-plant RL adapts to visuomotor rotations but not other BCIs.}
The motivation behind a simulator that incorporates RL adaptation is to discriminate between decoders that are suboptimal in ways that the policy can compensate for, vs those that are not. A basic sanity check here is to verify that in visuomotor rotations, which primates can readily compensate for~\citep{Chase2012BCIVisuomotorAdaptation}, can also be adapted to by our policy. We thus train a velocity control native MLP policy, and fit a linear BCI velocity decoder (which has perfect fit and recovery of the true action head, as expected)~\figref{fig:90_tune}. We see, as matches intuition, that larger rotations take longer to adapt to in terms of sample efficiency, but the sample requirement is highly nonlinear in rotation angle~\figref{fig:wmp_success}.

% Even in this controlled case, we find that random linear BCI decoders show nearly no learnability. We can reason through this in a thought experiment. If the random linear readout probes an axis that is orthogonal to the native action space, then state noise that causes good BCI actions will be independent of state noise that drives native policy variability. There is thus no way for
For control, we should examine learnability of other classes of BCI decoders, as even random linear decoders. Anecdotally, these were not controllable, but we do not have data to support this.

For most parity with existing literature, we should (but did not) examine learnability of BCI decoders that are in-manifold vs out-of-manifold, i.e. define random decoders within dimensions of high activation variability.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/closed_loop.png}
  \caption{Closed-loop control of BCI zero-shot, and learning curves that do not improve after fine-tuning.}
  \label{fig:cl_failure}
  \end{figure}

As a simple example that BCI control and fine-tuning fails for more suboptimal decoders, we show closed-loop control when the BCI decoder is first added (policy zero-shot control of BCI) and the learning curves that do not improve~\figref{fig:cl_failure}.

% \subsection{Covariate decodability is not controllability}

% - Acceleration → Velocity, decodable but not learnable.
% - Attribution w the acceleration control model controllability != evaluability


% \subsection{Dual plant demonstration with a muscle-actuated plant}


% - Show that we don’t need learning to reproduce Liang results - task was too easy.


\section{Discussion}
\label{sec:discussion}

There are three concerns about the approach we use in this work, and it is unclear if these are merely technical limitations or fundamental. First: It is not clear what the consequences of our forced placement of the decoder in the penultimate layer vs other locations are. In general, the dynamics of RL, through the correlated-control learning done here, are unclear. Second: By definition, policy network representations will shift to do BCI adaptation, leaving the interpretation of the original policy unambiguous. Representation preserving losses or native policy task training should anchor these representations. Care should be taken that the induced representations preserve the original distribution of activations, to be compatible with empirically observed re-association phenomena~\citep{golub2018learning,Humphreys2022GradientBCI}. Third: Though our design was motivated by the re-aiming hypothesis, its implementation implies plasticity of the motor cortex as the source of BCI adaptation, which contradicts the motivating work~\citep{Menendez2025LowDimControlTheory}, which instead motivates optimization of inputs to the motor module. An approach more commensurate with this theory (and~\citet{Liang2024BCISimulator}'s existing pipeline) would be a modular motor-input module that controls concrete command kinematics, and a BCI decoder that translates these kinematics to plant actuator (i.e. muscle) activations.
% Underlying contradiction: Why should people be able to "sample" behaviors at different, discrete levels of abstraction? That seems both unaesthetic and arbitrarily discrete.

The simulator currently recovers two qualitative facts about BCI adaptation: structured perturbations are learnable with modest tuning, while unstructured decoders are not. It fails in several other ways:
Human users can adapt to linear decoders that are far from perfect, but our policy cannot overcome even modest errors. Further, we do not expect decoder predicitivity of velocity to be so low given an unnoised, complete view of network hidden state, implying we are either not accessing the right part of the network, or our task pretraining is insufficient to induce the right representations.

The fulfilled vision of this project still has several points worth discussing.
Potential use cases:
- Discriminating learning algorithms for BCI adaptation. If we can instantiate this framework with multiple different learning algorithms, and then use those to create discrminative rankings for different BCI decoders, we can identify the learning algorithms most compatible with human adaptation by comparing rankings against real world experiments.

- We distinguish this from the more amibitious goal of a simulation that recreates more challenges in BCI decoding, such as accounting for its various nonstationarities; we thus are not pursuing a Sim2Real direct model translation of a BCI decoder.


Even as is, the project will only regards to translatability to real-world experiments.
- Thus any full release of this work (beyond this report) should include a corresponding validation and ideally verified novel prediction in real world experiments.

\bibliographystyle{plainnat}
\bibliography{midterm_update/main}

\end{document}
